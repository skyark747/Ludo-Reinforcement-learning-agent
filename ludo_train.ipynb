{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee466be3",
      "metadata": {
        "id": "ee466be3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from milestone2 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b1e53c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0b1e53c",
        "outputId": "cca70454-9e0d-47f2-a505-3d7ae39a6646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "STARTING = -1\n",
        "DESTINATION = 56\n",
        "SAFE_SQUARES = [0, 8, 13, 21, 25, 26, 34, 39, 47, 51, 52, 53, 54, 55, 56]\n",
        "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669ff280",
      "metadata": {
        "id": "669ff280"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a304ba44",
      "metadata": {
        "id": "a304ba44"
      },
      "outputs": [],
      "source": [
        "class Ludo:\n",
        "    def __init__(self, render_mode=\"\"):\n",
        "\n",
        "        self.all_gotis = [Gotis(\"red\"), Gotis(\"yellow\")]\n",
        "        self.dice = Dice()\n",
        "        self.terminated = False\n",
        "        self.player_turn = 0\n",
        "        self.roll = self.dice.roll()\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    def __repr__(self):\n",
        "        gotis_repr = \"\\n\\n\".join([repr(g) for g in self.all_gotis])\n",
        "        return (\n",
        "            f\"{gotis_repr}\\n\\n\"\n",
        "            f\"Dice Roll: {self.roll}\\n\"\n",
        "            f\"Terminated: {self.terminated}\\n\"\n",
        "            f\"Player Turn: {self.player_turn}\"\n",
        "        )\n",
        "\n",
        "    def step(self, action=None):\n",
        "        if self.terminated:\n",
        "            return self._get_state()\n",
        "\n",
        "        if self._no_action_possible(action):\n",
        "            return self._change_turn()\n",
        "\n",
        "        if not self._is_valid_input(action):\n",
        "            return self._handle_invalid_action()\n",
        "\n",
        "        return self._perform_move(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__()\n",
        "        return self._get_state()\n",
        "\n",
        "    def get_action_space(self):\n",
        "\n",
        "        gotis = (\n",
        "            self.all_gotis[0].gotis\n",
        "            if self.player_turn == 0\n",
        "            else self.all_gotis[1].gotis\n",
        "        )\n",
        "\n",
        "        action_space = [\n",
        "            (dice_index, goti_index)\n",
        "            for dice_index, dice in enumerate(self.roll)\n",
        "            for goti_index, goti in enumerate(gotis)\n",
        "            if self._is_valid_move(goti, dice)\n",
        "        ]\n",
        "\n",
        "        return action_space\n",
        "\n",
        "    def check_win(self, gotis):\n",
        "        gotis = gotis.gotis\n",
        "        return all(goti.position == 56 for goti in gotis)\n",
        "\n",
        "    def _perform_move(self, action):\n",
        "        dice_index, goti_number = action\n",
        "        dice = self.roll[dice_index]\n",
        "        self.roll.pop(dice_index)\n",
        "\n",
        "        self.all_gotis[self.player_turn].move_goti(goti_number, dice)\n",
        "\n",
        "        current_goti = self.all_gotis[self.player_turn].gotis[goti_number]\n",
        "        current_goti_position_opponent_view = (\n",
        "            current_goti.convert_into_opponent_position()\n",
        "        )\n",
        "\n",
        "        if self.all_gotis[not self.player_turn].kill_goti(\n",
        "            current_goti_position_opponent_view\n",
        "        ):\n",
        "            return self._get_extra_turn()\n",
        "\n",
        "        if self.check_win(self.all_gotis[self.player_turn]):\n",
        "            return self._handle_win()\n",
        "\n",
        "        if current_goti.position == DESTINATION:\n",
        "            return self._get_extra_turn()\n",
        "\n",
        "        if len(self.roll) >= 1:\n",
        "            return self._get_state()\n",
        "\n",
        "        return self._change_turn()\n",
        "\n",
        "    def _no_action_possible(self, action):\n",
        "        return action is None and not self.get_action_space()\n",
        "\n",
        "\n",
        "    def _is_valid_move(self, goti, dice):\n",
        "        if goti.position == STARTING:\n",
        "            return dice == 6\n",
        "        return goti.position + dice <= DESTINATION\n",
        "\n",
        "    def _is_valid_input(self, action):\n",
        "        return action in self.get_action_space()\n",
        "\n",
        "\n",
        "    def _change_turn(self):\n",
        "        self.player_turn = not self.player_turn\n",
        "        self.roll = self.dice.roll()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _handle_invalid_action(self):\n",
        "        self.terminated = True\n",
        "        self.player_turn = not self.player_turn\n",
        "        return self._get_state()\n",
        "\n",
        "    def _handle_win(self):\n",
        "        self.terminated = True\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_extra_turn(self):\n",
        "        new_roll = self.dice.roll()\n",
        "        self.roll += new_roll\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        return (\n",
        "            self.all_gotis[0],\n",
        "            self.all_gotis[1],\n",
        "            self.roll,\n",
        "            self.terminated,\n",
        "            self.player_turn,\n",
        "        )\n",
        "\n",
        "\n",
        "class Gotis:\n",
        "    def __init__(self, color: str):\n",
        "        self.color = color.capitalize()\n",
        "        self.gotis = [Goti() for _ in range(4)]\n",
        "\n",
        "    def __repr__(self):\n",
        "        goti_positions = \"\\n\".join(\n",
        "            [f\" Goti {i + 1}: {goti.position}\" for i, goti in enumerate(self.gotis)]\n",
        "        )\n",
        "        return f\"{self.color} Gotis' Distance from starting point:\\n{goti_positions}\"\n",
        "\n",
        "    def move_goti(self, goti_number, dice):\n",
        "        self.gotis[goti_number].move(dice)\n",
        "\n",
        "    def kill_goti(self, position):\n",
        "        for i in range(4):\n",
        "            if self.gotis[i].position == position:\n",
        "                if self.gotis[i].kill_goti():\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class Goti:\n",
        "    def __init__(self, position=STARTING):\n",
        "        self.position = position\n",
        "        assert STARTING <= self.position <= DESTINATION\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Goti's Distance from starting point: {self.position}\"\n",
        "\n",
        "    def move(self, dice):\n",
        "        if self.position == STARTING:\n",
        "            if dice == 6:\n",
        "                self.position = 0\n",
        "            return\n",
        "\n",
        "        if self.position + dice <= DESTINATION:\n",
        "            self.position += dice\n",
        "\n",
        "    def convert_into_opponent_position(self):\n",
        "        if STARTING >= self.position or self.position > 50 or self.position == 25:\n",
        "            return -2  # Position cannot be converted\n",
        "\n",
        "        if self.position <= 24:\n",
        "            return self.position + 26\n",
        "\n",
        "        return self.position - 26\n",
        "\n",
        "    def kill_goti(self):\n",
        "        if self.position not in SAFE_SQUARES:\n",
        "            self.position = -1\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class Dice:\n",
        "    def roll(self):\n",
        "\n",
        "        rolls = []\n",
        "\n",
        "        for _ in range(3):\n",
        "            roll = self.simulate_one_dice_roll()\n",
        "            rolls.append(roll)\n",
        "\n",
        "            if roll != 6:\n",
        "                break\n",
        "            elif len(rolls) == 3 and roll == 6:\n",
        "                return []\n",
        "\n",
        "        return rolls\n",
        "\n",
        "    def simulate_one_dice_roll(self):\n",
        "        return random.randint(1, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0e7d76",
      "metadata": {
        "id": "2c0e7d76"
      },
      "source": [
        "## Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fe214672",
      "metadata": {
        "id": "fe214672"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, actor_input_dim,critic_input_dim,actor_output_dim,critic_output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.actor= torch.nn.Sequential(\n",
        "            torch.nn.Linear(actor_input_dim,256),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(256,actor_output_dim),\n",
        "\n",
        "        )\n",
        "\n",
        "        self.critic= torch.nn.Sequential(\n",
        "            torch.nn.Linear(critic_input_dim,256),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(256,critic_output_dim),\n",
        "\n",
        "        )\n",
        "\n",
        "        self.optimizer=torch.optim.Adam(self.parameters(),lr=0.0001,betas=(0.9,0.999),eps=1e-8,weight_decay=0.01)\n",
        "        self.scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer,30000,1e-6)\n",
        "\n",
        "    def forward_actor(self,state,device=None):\n",
        "\n",
        "        if not torch.is_tensor(state):\n",
        "            state = torch.tensor(state, dtype=torch.float32)\n",
        "\n",
        "        if device:\n",
        "            state=state.to(device)\n",
        "        else:\n",
        "            state=state.to(DEVICE)\n",
        "\n",
        "        actions_prob = self.actor(state)\n",
        "        return actions_prob\n",
        "\n",
        "    def forward_critic(self,state,device=None):\n",
        "\n",
        "        if not torch.is_tensor(state):\n",
        "            state = torch.tensor(state, dtype=torch.float32)\n",
        "        if device:\n",
        "            state=state.to(device)\n",
        "        else:\n",
        "            state=state.to(DEVICE)\n",
        "        value = self.critic(state)\n",
        "        return value\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d324827",
      "metadata": {},
      "source": [
        "## Heuristic agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lJdC5GCm0L8k",
      "metadata": {
        "id": "lJdC5GCm0L8k"
      },
      "outputs": [],
      "source": [
        "class HeuristicAgent:\n",
        "    \n",
        "    def __init__(self, rng_seed=None):\n",
        "        if rng_seed is not None:\n",
        "            random.seed(rng_seed)\n",
        "\n",
        "    def score_action(self, env, action):\n",
        "        sim = copy.deepcopy(env)\n",
        "        next_state = sim.step(action)\n",
        "        reward = 0.0\n",
        "        if next_state[3] and next_state[4] == env.player_turn:\n",
        "            reward += 100.0\n",
        "            return reward\n",
        "        opp_index = 1 - env.player_turn\n",
        "        before_kills = sum(1 for g in env.all_gotis[opp_index].gotis if g.position == -1)\n",
        "        after_kills = sum(1 for g in next_state[opp_index].gotis if g.position == -1)\n",
        "        if after_kills > before_kills:\n",
        "            reward += 50.0\n",
        "\n",
        "        own_index = env.player_turn\n",
        "        for i in range(4):\n",
        "            before = env.all_gotis[own_index].gotis[i].position\n",
        "            after = next_state[own_index].gotis[i].position\n",
        "            if before == -1 and after == -1:\n",
        "                continue\n",
        "            if before ==  STARTING and after == 0:\n",
        "                reward += 10.0\n",
        "\n",
        "        progress = 0\n",
        "        for i in range(4):\n",
        "            pos_after = next_state[own_index].gotis[i].position\n",
        "            if pos_after == STARTING:\n",
        "                continue\n",
        "            if pos_after == -1:\n",
        "                continue\n",
        "            if pos_after == DESTINATION:\n",
        "                reward += 30.0\n",
        "            if pos_after in SAFE_SQUARES:\n",
        "                reward += 8.0\n",
        "            before_pos = env.all_gotis[own_index].gotis[i].position\n",
        "            delta = 0\n",
        "            if before_pos != STARTING and pos_after != STARTING and pos_after >= before_pos:\n",
        "                delta = pos_after - before_pos\n",
        "            reward += 0.5 * delta\n",
        "            if pos_after > progress:\n",
        "                progress = pos_after\n",
        "\n",
        "        reward += random.random() * 0.01\n",
        "        return reward\n",
        "\n",
        "    def get_action(self, env):\n",
        "        action_space = env.get_action_space()\n",
        "        if not action_space:\n",
        "            return None\n",
        "        # score each action and choose max\n",
        "        scored = [(self.score_action(env, a), a) for a in action_space]\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        return scored[0][1]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312d9814",
      "metadata": {
        "id": "312d9814"
      },
      "source": [
        "## Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e803945",
      "metadata": {
        "id": "8e803945"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_win_percentages(n, policy1, policy2):\n",
        "\n",
        "    env = Ludo()\n",
        "    wins = [0, 0]\n",
        "    policies = [policy1, policy2]\n",
        "\n",
        "    for i in range(2):\n",
        "        for _ in tqdm(range(n // 2)):\n",
        "\n",
        "            state = env.reset()\n",
        "            terminated = False\n",
        "            player_turn = 0\n",
        "\n",
        "            while not terminated:\n",
        "\n",
        "                action_space = env.get_action_space()\n",
        "                action = policies[player_turn].get_action(state, action_space)\n",
        "\n",
        "                state = env.step(action)\n",
        "                terminated, player_turn = state[3], state[4]\n",
        "\n",
        "            wins[player_turn - i] += 1\n",
        "\n",
        "        policies[0], policies[1] = policies[1], policies[0]\n",
        "\n",
        "    win_percentages = [(win / n) * 100 for win in wins]\n",
        "\n",
        "    return win_percentages\n",
        "\n",
        "\n",
        "class Policy_Random:\n",
        "    def get_action(self, state, action_space):\n",
        "        if action_space:\n",
        "            return random.choice(action_space)\n",
        "        return None\n",
        "\n",
        "\n",
        "class testpolicy():\n",
        "    def __init__(self):\n",
        "\n",
        "        self.q_net=Qnet(8,1).to(DEVICE)\n",
        "\n",
        "        best_weight=torch.load(r\"best.pth\")\n",
        "        self.q_net.load_state_dict(best_weight['model_state_dict'])\n",
        "\n",
        "    def get_action(self,state,action_space):\n",
        "        if len(action_space)==0:\n",
        "            return None\n",
        "\n",
        "        state_action_feats,_=self.get_state_values(state,action_space)\n",
        "\n",
        "        prob=self.actor_critic.forward_actor(state)\n",
        "\n",
        "        prob=torch.softmax(prob,dim=0)\n",
        "\n",
        "        prob=prob.squeeze(-1)\n",
        "\n",
        "        action_idx=torch.argmax(prob).item()\n",
        "        action = action_space[action_idx]\n",
        "\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_state_action_features(self,state,action):\n",
        "\n",
        "        dice_index,goti_index=action\n",
        "\n",
        "        red_gotis,yellow_gotis,dice_roll,terminated,player_id=state\n",
        "\n",
        "        if int(player_id)==0:\n",
        "            my_goti=red_gotis.gotis[goti_index]\n",
        "            dushman_goti=yellow_gotis.gotis\n",
        "        else:\n",
        "            my_goti=yellow_gotis.gotis[goti_index]\n",
        "            dushman_goti=red_gotis.gotis\n",
        "\n",
        "        dice_value=dice_roll[dice_index]\n",
        "\n",
        "        my_goti_pos=my_goti.position\n",
        "        old_goti_pos=my_goti_pos\n",
        "\n",
        "        if my_goti_pos==-1 and dice_value==6:\n",
        "            my_goti_pos=0\n",
        "        else:\n",
        "            my_goti_pos=my_goti_pos+dice_value if my_goti_pos+dice_value<=DESTINATION else my_goti_pos\n",
        "\n",
        "\n",
        "        # is in danger\n",
        "        is_in_danger=0\n",
        "        for dushman in dushman_goti:\n",
        "            if dushman.position != -1:\n",
        "                if 1<=my_goti_pos-dushman.position<=6:\n",
        "                    is_in_danger=1\n",
        "                    break\n",
        "\n",
        "\n",
        "        # can kill\n",
        "        can_kill=0\n",
        "        for dushman in dushman_goti:\n",
        "            if dushman.position!=-1 and my_goti_pos+dice_value == dushman.position:\n",
        "                can_kill=1\n",
        "                break\n",
        "\n",
        "        # is safe\n",
        "        is_safe=0\n",
        "        if my_goti_pos in SAFE_SQUARES:\n",
        "            is_safe=1\n",
        "\n",
        "\n",
        "        # distance from home\n",
        "        dist_home=1\n",
        "        if my_goti_pos!=-1:\n",
        "            dist_home=(DESTINATION-my_goti_pos)/DESTINATION\n",
        "\n",
        "\n",
        "\n",
        "        # is on home path\n",
        "        is_home_path=0\n",
        "        if (DESTINATION-my_goti_pos) <=5:\n",
        "            is_home_path=1\n",
        "\n",
        "\n",
        "        # can enter board\n",
        "        can_enter=0\n",
        "        if old_goti_pos == -1 and dice_value==6:\n",
        "            can_enter=1\n",
        "\n",
        "\n",
        "        # progress\n",
        "        progress=0\n",
        "        if my_goti_pos !=-1:\n",
        "            progress=(DESTINATION-my_goti_pos)/DESTINATION\n",
        "\n",
        "\n",
        "        return [is_in_danger,can_kill,is_safe,dist_home,is_home_path,can_enter,progress,dice_value/6]\n",
        "\n",
        "    def get_state_features(self,gotis):\n",
        "\n",
        "        g_home,g_goal,g_safe,min_dist,max_dist=0,0,0,float('inf'),float('-inf')\n",
        "        for g in gotis.gotis:\n",
        "            if g.position == -1:\n",
        "                max_dist=0\n",
        "                min_dist=0\n",
        "            if g.position == DESTINATION:\n",
        "                min_dist=0\n",
        "                max_dist=0\n",
        "\n",
        "            if g.position == -1:\n",
        "                g_home+=1\n",
        "\n",
        "            if g.position in SAFE_SQUARES:\n",
        "                g_safe+=1\n",
        "\n",
        "            if g.position == DESTINATION:\n",
        "                g_goal+=1\n",
        "\n",
        "            if g.position != -1 and g.position not in SAFE_SQUARES:\n",
        "                dist=(DESTINATION-g.position)/56.0\n",
        "\n",
        "                if dist < min_dist:\n",
        "                    min_dist=dist\n",
        "\n",
        "                elif dist > max_dist:\n",
        "                    max_dist=dist\n",
        "\n",
        "\n",
        "        return g_home,g_goal,g_safe,min_dist,max_dist\n",
        "\n",
        "    def get_state_values(self,state,action_space):\n",
        "\n",
        "        gotis_red,gotis_yellow,dice_roll,terminated,player_turn=state\n",
        "\n",
        "        r_home,r_goal,r_safe,r_min_dist,r_max_dist=self.get_state_features(gotis_red)\n",
        "        y_home,y_goal,y_safe,y_min_dist,y_max_dist=self.get_state_features(gotis_yellow)\n",
        "\n",
        "        r_active=4-(r_home+r_goal)\n",
        "        y_active=4-(y_home+y_goal)\n",
        "\n",
        "        # 1. whether any 6 exists in the roll\n",
        "        has_six = 1 if 6 in dice_roll else 0\n",
        "\n",
        "        # 2. number of sixes in the roll\n",
        "        num_sixes = dice_roll.count(6)\n",
        "\n",
        "      \n",
        "\n",
        "        state_key=[r_home,r_goal,r_active,r_safe,y_home,y_goal,y_active,y_safe,has_six,num_sixes,sum(dice_roll)]\n",
        "\n",
        "        state_key=np.array(state_key)\n",
        "\n",
        "        state_key=(state_key-np.mean(state_key)/np.std(state_key))\n",
        "        state_key=(state_key-np.min(state_key))/(np.max(state_key)-np.min(state_key))\n",
        "\n",
        "\n",
        "        features=[]\n",
        "        for a in action_space:\n",
        "            features.append(self.get_state_action_features(state,a))\n",
        "\n",
        "        features=np.array(features)\n",
        "\n",
        "        features=(features-np.mean(features)/np.std(features))\n",
        "        features=(features-np.min(features))/(np.max(features)-np.min(features))\n",
        "\n",
        "\n",
        "        return features,state_key\n",
        "\n",
        "\n",
        "class policy():\n",
        "    def __init__(self,epsilon,alpha,gamma):\n",
        "        self.env=Ludo()\n",
        "        self.epsilon=epsilon\n",
        "        self.alpha=alpha\n",
        "        self.gamma=gamma\n",
        "        self.q_table=dict()\n",
        "        self.Model=dict()\n",
        "        self.current_state=0\n",
        "        self.actor_critic=ActorCritic(8,11,1,1)\n",
        "        \n",
        "\n",
        "    def update_q_table(self,state,action_space):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state]={}\n",
        "            for action in action_space:\n",
        "                if action not in self.q_table[state]:\n",
        "                    self.q_table[state][action]=0.0\n",
        "\n",
        "    def save_q_table(self,name):\n",
        "        np.save(name,self.q_table)\n",
        "\n",
        "    def get_action_eval(self,state,action_space):\n",
        "\n",
        "        if len(action_space)==0:\n",
        "            return None\n",
        "\n",
        "        state_action_feats,_=self.get_state_values(state,action_space)\n",
        "        \n",
        "        self.actor_critic.eval()\n",
        "        with torch.no_grad():\n",
        "               prob=self.actor_critic.forward_actor(state_action_feats)\n",
        "\n",
        "               prob=prob.squeeze(-1)\n",
        "\n",
        "        prob=torch.softmax(prob,dim=0)\n",
        "\n",
        "        action_idx = torch.argmax(prob).item()\n",
        "        action = action_space[action_idx]\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_state_action_features(self,state,action):\n",
        "\n",
        "        dice_index,goti_index=action\n",
        "\n",
        "        red_gotis,yellow_gotis,dice_roll,terminated,player_id=state\n",
        "        \n",
        "        my_goti=red_gotis.gotis[goti_index]\n",
        "        dushman_goti=yellow_gotis.gotis\n",
        "        \n",
        "        dice_value=dice_roll[dice_index]\n",
        "\n",
        "        my_goti_pos=my_goti.position\n",
        "        old_goti_pos=my_goti_pos\n",
        "\n",
        "        if my_goti_pos==-1 and dice_value==6:\n",
        "            my_goti_pos=0\n",
        "        else:\n",
        "            my_goti_pos=my_goti_pos+dice_value if my_goti_pos+dice_value<=DESTINATION else my_goti_pos\n",
        "\n",
        "\n",
        "        # is in danger\n",
        "        is_in_danger=0\n",
        "        for dushman in dushman_goti:\n",
        "            if dushman.position != -1:\n",
        "                if 1<=my_goti_pos-dushman.position<=6:\n",
        "                    is_in_danger=1\n",
        "                    break\n",
        "\n",
        "\n",
        "        # can kill\n",
        "        can_kill=0\n",
        "        for dushman in dushman_goti:\n",
        "            if dushman.position!=-1 and my_goti_pos+dice_value == dushman.position:\n",
        "                can_kill=1\n",
        "                break\n",
        "\n",
        "        # is safe\n",
        "        is_safe=0\n",
        "        if my_goti_pos in SAFE_SQUARES:\n",
        "            is_safe=1\n",
        "\n",
        "\n",
        "        # distance from home\n",
        "        dist_home=1\n",
        "        if my_goti_pos!=-1:\n",
        "            dist_home=(DESTINATION-my_goti_pos)/DESTINATION\n",
        "\n",
        "\n",
        "\n",
        "        # is on home path\n",
        "        is_home_path=0\n",
        "        if (DESTINATION-my_goti_pos) <=5:\n",
        "            is_home_path=1\n",
        "\n",
        "\n",
        "        # can enter board\n",
        "        can_enter=0\n",
        "        if old_goti_pos == -1 and dice_value==6:\n",
        "            can_enter=1\n",
        "\n",
        "\n",
        "        # progress\n",
        "        progress=0\n",
        "        if my_goti_pos !=-1:\n",
        "            progress=(DESTINATION-my_goti_pos)/DESTINATION\n",
        "\n",
        "\n",
        "        return [is_in_danger,can_kill,is_safe,dist_home,is_home_path,can_enter,progress,dice_value/6]\n",
        "\n",
        "    def get_state_features(self,gotis):\n",
        "\n",
        "        g_home,g_goal,g_safe,min_dist,max_dist=0,0,0,float('inf'),float('-inf')\n",
        "        for g in gotis.gotis:\n",
        "            if g.position == -1:\n",
        "                max_dist=0\n",
        "                min_dist=0\n",
        "            if g.position == DESTINATION:\n",
        "                min_dist=0\n",
        "                max_dist=0\n",
        "\n",
        "            if g.position == -1:\n",
        "                g_home+=1\n",
        "\n",
        "            if g.position in SAFE_SQUARES:\n",
        "                g_safe+=1\n",
        "\n",
        "            if g.position == DESTINATION:\n",
        "                g_goal+=1\n",
        "\n",
        "            if g.position != -1 and g.position not in SAFE_SQUARES:\n",
        "                dist=(DESTINATION-g.position)/56.0\n",
        "\n",
        "                if dist < min_dist:\n",
        "                    min_dist=dist\n",
        "\n",
        "                elif dist > max_dist:\n",
        "                    max_dist=dist\n",
        "\n",
        "        return g_home,g_goal,g_safe,min_dist,max_dist\n",
        "\n",
        "    def get_state_values(self,state,action_space):\n",
        "\n",
        "        gotis_red,gotis_yellow,dice_roll,terminated,player_turn=state\n",
        "\n",
        "        r_home,r_goal,r_safe,r_min_dist,r_max_dist=self.get_state_features(gotis_red)\n",
        "        y_home,y_goal,y_safe,y_min_dist,y_max_dist=self.get_state_features(gotis_yellow)\n",
        "\n",
        "        r_active=4-(r_home+r_goal)\n",
        "        y_active=4-(y_home+y_goal)\n",
        "\n",
        "        # 1. whether any 6 exists in the roll\n",
        "        has_six = 1 if 6 in dice_roll else 0\n",
        "\n",
        "        # 2. number of sixes in the roll\n",
        "        num_sixes = dice_roll.count(6)\n",
        "\n",
        "\n",
        "        state_key=[r_home,r_goal,r_active,r_safe,y_home,y_goal,y_active,y_safe,has_six,num_sixes,sum(dice_roll)]\n",
        "\n",
        "        state_key=np.array(state_key)\n",
        "\n",
        "        state_key=(state_key-np.mean(state_key)/np.std(state_key))\n",
        "        state_key=(state_key-np.min(state_key))/(np.max(state_key)-np.min(state_key))\n",
        "\n",
        "\n",
        "        features=[]\n",
        "        for a in action_space:\n",
        "            features.append(self.get_state_action_features(state,a))\n",
        "\n",
        "        features=np.array(features)\n",
        "\n",
        "        features=(features-np.mean(features)/np.std(features))\n",
        "        features=(features-np.min(features))/(np.max(features)-np.min(features))\n",
        "\n",
        "\n",
        "        return features,state_key\n",
        "\n",
        "    def get_action(self,state,action_space):\n",
        "\n",
        "        if len(action_space)==0:\n",
        "            return None\n",
        "\n",
        "        r=np.random.rand()\n",
        "        if r<self.epsilon:\n",
        "            action=random.choice(action_space)\n",
        "        else:\n",
        "\n",
        "            prob=self.actor_critic.forward_actor(state)\n",
        "\n",
        "            prob=torch.softmax(prob,dim=0)\n",
        "\n",
        "            prob=prob.squeeze(-1)\n",
        "\n",
        "            action_idx=torch.argmax(prob).item()\n",
        "            action = action_space[action_idx]\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_reward(self, current_state, next_state,player_id):\n",
        "\n",
        "        prev_red, prev_yellow = current_state[0], current_state[1]\n",
        "        curr_red, curr_yellow = next_state[0], next_state[1]\n",
        "\n",
        "\n",
        "        my_goti=curr_red\n",
        "        my_goti_prev=prev_red.gotis\n",
        "        my_goti_curr=curr_red.gotis\n",
        "        dushman_goti_prev=prev_yellow.gotis\n",
        "        dushman_goti_curr=curr_yellow.gotis\n",
        "\n",
        "        \n",
        "        reward = 0\n",
        "\n",
        "        my_prev_goti_p = [g.position for g in my_goti_prev]\n",
        "        my_curr_goti_p = [g.position for g in my_goti_curr]\n",
        "        dushman_prev_p = [g.position for g in dushman_goti_prev]\n",
        "        dushman_curr_p = [g.position for g in dushman_goti_curr]\n",
        "\n",
        "        # move forward\n",
        "        for i in range(4):\n",
        "            if my_prev_goti_p[i] != -1 and my_curr_goti_p[i] != -1 and my_curr_goti_p[i]!=DESTINATION and my_prev_goti_p[i]!=my_curr_goti_p[i]:\n",
        "                reward += 0.01 \n",
        "\n",
        "        # get out of base\n",
        "        for i in range(4):\n",
        "            if my_prev_goti_p[i] == -1 and my_curr_goti_p[i] >= 0:\n",
        "                reward += 0.1\n",
        "\n",
        "        # land on safe square\n",
        "        for i in range(4):\n",
        "              if my_curr_goti_p[i] in SAFE_SQUARES and my_prev_goti_p[i] not in SAFE_SQUARES:\n",
        "                  reward += 0.1\n",
        "\n",
        "        # kill opponent\n",
        "        for r in range(4):\n",
        "              for y in range(4):\n",
        "                  if my_curr_goti_p[r] == dushman_prev_p[y] and dushman_prev_p[y] not in SAFE_SQUARES:\n",
        "                      reward += 0.3\n",
        "\n",
        "        # if own guiti captured\n",
        "        for r in range(4):\n",
        "            for y in range(4):\n",
        "                if dushman_curr_p[y] == my_prev_goti_p[r] and my_prev_goti_p[r] not in SAFE_SQUARES:\n",
        "                    reward -= 0.4\n",
        "\n",
        "        # get to destination\n",
        "        for i in range(4):\n",
        "            if my_curr_goti_p[i] == DESTINATION and my_prev_goti_p[i] != DESTINATION:\n",
        "                reward += 0.5\n",
        "\n",
        "        if next_state[3]:  # terminated\n",
        "            if self.env.check_win(my_goti):\n",
        "                reward += 1.0\n",
        "            else:\n",
        "                reward -= 1.0\n",
        "\n",
        "\n",
        "        return reward\n",
        "\n",
        "    \n",
        "    def actor_critic(self,episodes,_lambda):\n",
        "        best_res=0\n",
        "        self.actor_critic.to(DEVICE)\n",
        "        opp=HeuristicAgent()\n",
        "        for episode in range(episodes):\n",
        "            self.current_state=self.env.reset()\n",
        "            terminated=self.current_state[3]\n",
        "            running_loss=0.0\n",
        "            player_turn=self.current_state[4]\n",
        "\n",
        "            states_buf = []\n",
        "            action_feats_buf = []\n",
        "            actions_buf = []\n",
        "            rewards_buf = []\n",
        "            values_buf = []\n",
        "            masks_buf = []\n",
        "\n",
        "            total_rewards=0.0\n",
        "\n",
        "            while not terminated:\n",
        "                action_space=self.env.get_action_space()\n",
        "\n",
        "\n",
        "                if len(action_space)==0:\n",
        "                    next_state=self.env.step(None)\n",
        "                    terminated=next_state[3]\n",
        "                    self.current_state=next_state\n",
        "                    player_turn=self.current_state[4]\n",
        "                    continue\n",
        "\n",
        "\n",
        "                if int(player_turn)==1:\n",
        "                    action=opp.get_action(self.env)\n",
        "                    next_state = self.env.step(action)\n",
        "                    terminated = next_state[3]\n",
        "                    player_turn=next_state[4]\n",
        "                    self.current_state = next_state\n",
        "\n",
        "\n",
        "\n",
        "                else:\n",
        "\n",
        "\n",
        "                    state_action_feats,state_feats=self.get_state_values(self.current_state,action_space)\n",
        "\n",
        "\n",
        "                    action=self.get_action(state_action_feats,action_space)\n",
        "\n",
        "\n",
        "                    # critic\n",
        "                    value=self.actor_critic.forward_critic(state_feats)\n",
        "                    value=value.squeeze()\n",
        "\n",
        "\n",
        "                    next_state=self.env.step(action)\n",
        "\n",
        "                    reward=self.get_reward(self.current_state,next_state,player_turn)\n",
        "\n",
        "                    terminated=next_state[3]\n",
        "                    player_turn=next_state[4]\n",
        "                    self.current_state=next_state\n",
        "\n",
        "                    states_buf.append(state_feats)\n",
        "                    action_feats_buf.append(state_action_feats)\n",
        "                    actions_buf.append(action_space.index(action))\n",
        "                    rewards_buf.append(reward)\n",
        "                    values_buf.append(value)\n",
        "                    masks_buf.append(0 if terminated else 1)\n",
        "\n",
        "                    total_rewards+=reward\n",
        "\n",
        "                    if len(rewards_buf) >= _lambda or terminated:\n",
        "                        next_action_space=self.env.get_action_space()\n",
        "\n",
        "                        if terminated or len(next_action_space)==0:\n",
        "                            next_value=0.0\n",
        "\n",
        "                        else:\n",
        "                            next_state_act_feats,next_state_features=self.get_state_values(next_state,next_action_space)\n",
        "\n",
        "                            next_value=self.q_net.forward()\n",
        "\n",
        "                            next_value=next_value.squeeze(-1)\n",
        "\n",
        "                        returns=next_value\n",
        "                        for r, m in zip(reversed(rewards_buf), reversed(masks_buf)):\n",
        "\n",
        "                            returns = r + (self.gamma) * returns * m\n",
        "\n",
        "                        value = values_buf[0]\n",
        "                        advantage = returns - value\n",
        "\n",
        "                        # actor loss\n",
        "                        probs = self.actor_critic.forward_actor(action_feats_buf[0])\n",
        "                        probs=torch.softmax(probs,dim=0)\n",
        "\n",
        "                        act_idx = actions_buf[0]\n",
        "\n",
        "                        actor_loss = -torch.log(probs[act_idx] + 1e-8) * advantage\n",
        "\n",
        "                        # critic loss\n",
        "                        critic_loss = advantage**2\n",
        "\n",
        "                        loss = actor_loss + critic_loss\n",
        "\n",
        "\n",
        "                        self.actor_critic.optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        self.actor_critic.optimizer.step()\n",
        "\n",
        "                        states_buf.clear()\n",
        "                        action_feats_buf.clear()\n",
        "                        actions_buf.clear()\n",
        "                        rewards_buf.clear()\n",
        "                        values_buf.clear()\n",
        "                        masks_buf.clear()\n",
        "\n",
        "                        running_loss+=loss.item()\n",
        "                \n",
        "            self.actor_critic.scheduler.step()\n",
        "\n",
        "            self.epsilon = max(self.epsilon * 0.99995, 0.05)\n",
        "\n",
        "            with open(\"episode_rewards.log\", \"a\") as f:\n",
        "                f.write(f\"{episode},{total_rewards}\\n\")\n",
        "\n",
        "            if episode % 500==0:\n",
        "                print(f\"episodes: [{episode}/{episodes}] , epsilon: [{self.epsilon}], lr: [{self.actor_critic.scheduler.get_last_lr()}] , loss(actor_critic): [{running_loss/episodes}]\")\n",
        "\n",
        "            if episode==990:\n",
        "                torch.save({\"model_state_dict\":self.actor_critic.state_dict()},'best1.pth')\n",
        "\n",
        "            if episode % 1000==0 and episode>0:\n",
        "\n",
        "                win_percentage=get_win_percentages(1000,testpolicy(),Policy_Random())\n",
        "                print(win_percentage)\n",
        "\n",
        "                if win_percentage[0]>=best_res:\n",
        "                    best_res=win_percentage[0]\n",
        "                    torch.save({\"model_state_dict\":self.actor_critic.state_dict()},'best1.pth')\n",
        "                    print(\"best model saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d553be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6d553be",
        "outputId": "47bb926d-fdd9-49a5-986e-e682d218cfd7"
      },
      "outputs": [],
      "source": [
        "env=policy(0.1,0.0001,0.95)\n",
        "env.Q_net_app(30000,5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RLenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
